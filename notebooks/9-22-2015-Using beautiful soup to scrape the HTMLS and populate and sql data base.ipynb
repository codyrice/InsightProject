{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goals: \n",
    "In this notebook I am attempting to do the following: \n",
    "\n",
    "1. Set up an SQL table with sql alchemy. \n",
    "2. Use the the Beautiful soup parsers that I have written to parse the html and then place into a database. \n",
    "\n",
    "I had to remake one of the tables. \n",
    "\n",
    "### First import the libraries \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# modules\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'   #this makes it retina style (oh yeah!)\n",
    "from bs4 import BeautifulSoup\n",
    "from os.path import join\n",
    "from pandas import DataFrame\n",
    "from os import getcwd, listdir\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "#append the path for my own funtions and import them \n",
    "import sys\n",
    "sys.path.append('/Users/Cody/Documents/InsightProject')\n",
    "from scraper import * \n",
    "from data_functions import *\n",
    "\n",
    "\n",
    "from multiprocessing.pool import ThreadPool as Pool\n",
    "from validators.url import url as validate_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sqlalchemy as sa\n",
    "from sqlalchemy import Column, Table, MetaData,create_engine\n",
    "from sqlalchemy.dialects.mysql import *\n",
    "from sqlalchemy.sql.expression import insert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use sqlalchemy to establish a connection to the database. \n",
    "In sql server I created a database called url. I also had to install and configure the SQL database. I am now going to use this sql data base to contain my strings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "engine = sa.create_engine('mysql://root:@localhost/url')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results= engine.execute('show databases').fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a table to hold the data for the scrapped websites\n",
    "1. First create a metadata object and bind it to the engine.\n",
    "2. Then make the table. This table hold an ID, a url (the string), and a medium text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metadata =MetaData()\n",
    "metadata.bind = engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used the following commands to create the tables\n",
    "\n",
    "```python\n",
    "\n",
    "url =Table('classifier_htmls',metadata,\n",
    "            Column('urlID', INTEGER, primary_key=True, autoincrement=True),\n",
    "            Column('url', VARCHAR(60)),\n",
    "            Column('htmlsText',MEDIUMTEXT),\n",
    "            Column('htmlsSize', INTEGER))\n",
    "\n",
    "\n",
    "student = Table('student_htmls',metadata,\n",
    "            Column('urlID', INTEGER, primary_key=True, autoincrement=True),\n",
    "            Column('url', VARCHAR(60)),\n",
    "            Column('htmlsText',MEDIUMTEXT),\n",
    "            Column('htmlsSize', INTEGER))\n",
    "            \n",
    "metadata.create_all(checkfirst=True)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "engine.execute('show tables').fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "engine.execute('describe student_htmls').fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now time to use the beautiful soup functions to start parsing. \n",
    "\n",
    "First I will write a function to be called by the multiprocessing pools. \n",
    "This function will: \n",
    "\n",
    "1. Import the text, and scrape it. \n",
    "2. It will calculate the number of strings for each text. \n",
    "3. It will then insert the url, text, and the length of the string into the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eng = create_engine('mysql://root:@localhost/url',pool_size=32)   #create a new engine with 32 connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "meta = MetaData()\n",
    "meta.bind = eng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier_htmls = Table('classifier_htmls',metadata,\n",
    "            Column('urlID', INTEGER, primary_key=True, autoincrement=True),\n",
    "            Column('url', VARCHAR(60)),\n",
    "            Column('htmlsText',MEDIUMTEXT),\n",
    "            Column('htmlsSize', INTEGER))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#``` python\n",
    "def parse_text(html):\n",
    "    \"\"\" parses the html and inserts into the database\"\"\"\n",
    "    \n",
    "    # parste the text and calculate the size\n",
    "    html_path = '/home/ubuntu/data/html'\n",
    "    url = html   #get the urginal url\n",
    "    html = join(html_path, html)\n",
    "    parsed = scrape_all_text(html)\n",
    "    size = len(parsed)\n",
    "    \n",
    "    # handle the insertion into the table. \n",
    "    eng.dispose()\n",
    "    with eng.connect() as conn:\n",
    "        # insert the data into the database. \n",
    "        ins = classifier_htmls.insert().values(url = url, htmlsText = parsed, htmlsSize=size)\n",
    "        conn.execute(ins)\n",
    "#    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "html_path = '/home/ubuntu/data/html'\n",
    "htmls= [html for html in listdir('/home/ubuntu/data/html')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'the number of entries is %d' %len(eng.execute('select * from classifier_htmls').fetchall())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` python i = parse_text(htmls[0]) ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#I used this script to parse the data. I am going to write a new fuction to scrape t\n",
    "\n",
    "#```python\n",
    "\n",
    "def insert_htmls(htmls,threads = 32):\n",
    "    \"\"\"inserts multoples\"\"\"\n",
    "    pool = Pool(threads)\n",
    "    pool.map(parse_text, htmls)\n",
    "    pool.close()   #```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#```python \n",
    "insert_htmls(htmls) #```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Add more the data that I previously downloaded to the data base. \n",
    "\n",
    "The goal of this part is to repeat what I did, scrape the remaing sites and input them into the sql database.  To do this. \n",
    "\n",
    "1. Set the path and file location. \n",
    "2. Modify the two fuctions. \n",
    "3. Run it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "htmls= [html for html in listdir('/home/ubuntu/data/GoGuardianHTMLS-9-17-2015-A')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parse_text(html):\n",
    "    \"\"\" parses the html and inserts into the database\"\"\"\n",
    "    \n",
    "    # parste the text and calculate the size\n",
    "    html_path = '/home/ubuntu/data/GoGuardianHTMLS-9-17-2015-A'\n",
    "    url = html   #get the urginal url\n",
    "    html = join(html_path, html)\n",
    "    parsed = scrape_all_text(html)\n",
    "    size = len(parsed)\n",
    "    \n",
    "    # handle the insertion into the table. \n",
    "    eng.dispose()\n",
    "    with eng.connect() as conn:\n",
    "        # insert the data into the database. \n",
    "        ins = classifier_htmls.insert().values(url = url, htmlsText = parsed, htmlsSize=size)\n",
    "        conn.execute(ins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def insert_htmls(htmls,threads = 32):\n",
    "    \"\"\"inserts multoples\"\"\"\n",
    "    pool = Pool(threads)\n",
    "    pool.map(parse_text, htmls)\n",
    "    pool.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```insert_htmls(htmls)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r = engine.execute('Select * from classifier_htmls').fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path1 = '/home/ubuntu/data/html'\n",
    "path2 = '/home/ubuntu/data/GoGuardianHTMLS-9-17-2015-A'\n",
    "len(listdir(path1)) +len(listdir(path2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_sql('Select * from classifier_htmls', con = engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
